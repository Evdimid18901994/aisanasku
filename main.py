import streamlit as st
import pandas as pd
import numpy as np
import joblib
import json
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, precision_score, recall_score, roc_auc_score, accuracy_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from imblearn.under_sampling import RandomUnderSampler
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve
import time
import zipfile
import io
import shutil

st.set_page_config(page_title='Antifraud AI', page_icon=':mag:', layout='wide', initial_sidebar_state = 'auto')

st.markdown("""
<style>
/* –¶–≤–µ—Ç –≤—Å–µ—Ö –ø—É–Ω–∫—Ç–æ–≤ */
[data-testid="stSidebarNavLink"]:hover {
    background-color: #C08BA5 !important; 
    color: white;/* –ü–µ–ø–µ–ª—å–Ω–æ-—Ä–æ–∑–æ–≤—ã–π */
    font-weight: 500;
    font-size: 16px;
    text-decoration: none;
}

[data-testid="stSidebarNavLink"]:hover span {
    
    color: white;/* –ü–µ–ø–µ–ª—å–Ω–æ-—Ä–æ–∑–æ–≤—ã–π */

}

[data-testid="stSidebarNavLink"]:focus {
    
    background-color: transparent; 
     /* –ü–µ–ø–µ–ª—å–Ω–æ-—Ä–æ–∑–æ–≤—ã–π */

}

[data-testid="stMainBlockContainer"] {
    padding: 2rem 2rem 2rem 2rem;
}

[data-testid="stFullScreenFrame"] {
    padding: 0rem 2rem 0rem 2rem;
    display:flex;
    justify-content: center;
}


[data-testid="stSidebar"] {
    min-width: 10vw;
    max-width: 60vw;
}

[data-testid="stSidebarUserContent"] p {
    font-size: 16px !important;
}

[data-testid="stSidebarUserContent"] {
    display: flex;
    margin:auto;
}


[data-testid="stSelectbox"] div{
    font-size: 16px;
}


</style>
""", unsafe_allow_html=True)

st.set_page_config(page_title='Antifraud AI', page_icon = "ammit_search.png", layout = 'wide', initial_sidebar_state = 'auto')

icon = "nku_icon.png"

#left_co, cent_co, last_co = st.columns([0.35, 0.3, 0.35])
#with cent_co:
 #   st.image("ammit.png")
#col1, col2 = st.columns([0.1, 0.9])
#with col1:
 #   st.image(icon, width=100)
#with col2:

st.title("–ê–Ω—Ç–∏—Ñ—Ä–æ–¥ ML")

st.sidebar.image("nku_icon.png", width=100)

if st.sidebar.button("–í–æ–π—Ç–∏"):
    st.login("google")

page = st.sidebar.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É:", ["üèãÔ∏è‚Äç‚ôÇÔ∏è –û–±—É—á–∏—Ç—å", "üîç –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å"])

if st.user.is_logged_in:
    def align_features(df, feature_names):
        for col in feature_names:
            if col not in df.columns:
                df[col] = 0
        return df[feature_names]

    if st.sidebar.button("–í—ã–π—Ç–∏"):
        st.logout()
    # Classifier models selection
    models = {
        "RandomForest": RandomForestClassifier(n_estimators=100, class_weight="balanced", random_state=42),
        "GradientBoosting": GradientBoostingClassifier(n_estimators=250, learning_rate=0.1, max_depth=6,
                                                       random_state=42),
        "CatBoost": CatBoostClassifier(verbose=0, random_state=42),
        "LightGBM": LGBMClassifier(random_state=42),
        "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric="logloss", random_state=42)
    }

    if page == "üèãÔ∏è‚Äç‚ôÇÔ∏è –û–±—É—á–∏—Ç—å":
        st.header("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏")
        st.subheader("–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        st.subheader("–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        col1, col2 = st.columns(2)
        with col1:
            num_files = st.slider(
                "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤",
                min_value=1,
                max_value=20,
                value=5
            )
        with col2:
            records_per_file = st.slider(
                "–ó–∞–ø–∏—Å–µ–π –≤ –∫–∞–∂–¥–æ–º —Ñ–∞–π–ª–µ",
                min_value=1000,
                max_value=100000,
                value=10000,
                step=1000
            )


        if st.button("üîÑ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"):
            def generate_realistic_data(n=10000, seed=None, fraud_pct=0.05):
                if seed is not None:
                    np.random.seed(seed)

                # –ü–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è –≤–∞—à–µ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
                data = {
                    "Age": np.random.randint(18, 80, size=n),
                    "Occupation": np.random.choice([
                        "Teacher", "Engineer", "Clerk", "Unemployed", "Software Developer",
                        "Manager", "Technician", "Consultant", "Analyst", "Sales"
                    ], size=n),
                    "MaritalStatus": np.random.choice(["Single", "Married", "Divorced"], size=n),
                    "Dependents": np.random.randint(0, 5, size=n),
                    "ResidentialStatus": np.random.choice(["Own", "Rent", "Live with Parents"], size=n),
                    "AddressDuration": np.random.randint(0, 360, size=n),
                    "CreditScore": np.random.normal(650, 100, size=n).clip(300, 850).astype(int),
                    "IncomeLevel": np.random.exponential(scale=50000, size=n).clip(5000, 200000).astype(int),
                    "LoanAmountRequested": np.random.gamma(shape=2, scale=50000, size=n).clip(1000, 500000).astype(int),
                    "LoanTerm": np.random.randint(1,30 ,size=n),
                    "PurposeoftheLoan": np.random.choice([
                        "Education", "Business", "Car", "House", "Medical", "Vacation"
                    ], size=n, p=[0.2, 0.3, 0.15, 0.25, 0.05, 0.05]),
                    "Collateral": np.random.choice(["House", "Car", "None"], size=n, p=[0.4, 0.3, 0.3]),
                    "InterestRate": np.round(np.random.normal(10, 3, size=n).clip(1.5, 25), 2),
                    "PreviousLoans": np.random.poisson(2, size=n).clip(0, 10),
                    "ExistingLiabilities": np.random.binomial(10, 0.3, size=n),
                    "ApplicationBehavior": np.random.choice(["Careful", "Risky"], size=n, p=[0.7, 0.3]),
                    "LocationofApplication": np.random.choice(["Online", "Branch", "Referral"], size=n,
                                                              p=[0.6, 0.3, 0.1]),
                    "ChangeinBehavior": np.random.choice(["Stable", "Sudden", "Gradual"], size=n, p=[0.8, 0.1, 0.1]),
                    "TimeofTransaction": np.random.choice(["Morning", "Afternoon", "Evening", "Night"], size=n,
                                                          p=[0.2, 0.3, 0.3, 0.2]),
                    "AccountActivity": np.random.choice(["Normal", "Unusual"], size=n, p=[0.9, 0.1]),
                    "PaymentBehavior": np.random.choice(["On-time", "Delayed", "Defaulted"], size=n,
                                                        p=[0.85, 0.1, 0.05]),
                    "Blacklists": np.random.choice(["Yes", "No"], size=n, p=[0.1, 0.9]),
                    "EmploymentVerification": np.random.choice(["Verified", "Not Verified"], size=n, p=[0.8, 0.2]),
                    "PastFinancialMalpractices": np.random.choice(["Yes", "No"], size=n, p=[0.05, 0.95]),
                    "DeviceInformation": np.random.choice(["Mobile", "Laptop", "Tablet"], size=n, p=[0.6, 0.3, 0.1]),
                    "SocialMediaFootprint": np.random.choice(["Yes", "No"], size=n, p=[0.7, 0.3]),
                    "ConsistencyinData": np.random.choice(["Consistent", "Inconsistent"], size=n, p=[0.9, 0.1]),
                    "Referral": np.random.choice(["Online", "Referral", "Branch"], size=n, p=[0.5, 0.3, 0.2]),
                }
                return pd.DataFrame(data)


            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
            progress_bar = st.progress(0)
            status_text = st.empty()

            os.makedirs("synthetic_data", exist_ok=True)

            for i in range(num_files):
                df = generate_realistic_data(
                    records_per_file,
                    seed=42 + i,
                )
                df.to_csv(f"synthetic_data/dataset_{i + 1}.csv", index=False)
                progress_bar.progress((i + 1) / num_files)
                status_text.text(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è {i + 1}/{num_files} —Ñ–∞–π–ª–æ–≤...")

            # –£–ø–∞–∫–æ–≤–∫–∞ –≤ ZIP
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w") as zipf:
                for i in range(1, num_files + 1):
                    with open(f"synthetic_data/dataset_{i}.csv", "rb") as f:
                        zipf.writestr(f"fraud_dataset_{i}.csv", f.read())

            # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
            st.download_button(
                label=f"üì• –°–∫–∞—á–∞—Ç—å {num_files} —Ñ–∞–π–ª–æ–≤ (ZIP)",
                data=zip_buffer.getvalue(),
                file_name="fraud_datasets.zip",
                mime="application/zip"
            )

            # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            shutil.rmtree("synthetic_data")
            st.success("‚úÖ –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã!")
        uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV —Å –∫–æ–ª–æ–Ω–∫–æ–π 'IsFraud'", type=["csv"])
        use_default = st.checkbox("–ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç")

        if use_default:
            uploaded_file = "./account_data.csv"
            st.info("‚úÖ –í—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞")

        model_name = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:", list(models.keys()))

        if uploaded_file:
            # Loading data
            df = pd.read_csv(uploaded_file)

            # –í—ã–±–æ—Ä –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é —Ñ–ª–∞–∂–∫–æ–≤
            selected_features = st.sidebar.multiselect(
                "üîß –û—Ç–º–µ—Ç—å—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è:",
                df.columns.tolist(),
                default=df.columns.tolist()
            )

            st.dataframe(df[selected_features])

            if st.button("üöÄ –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å"):
                # GB Flag = "IsFraud" column, binary classification task
                y = df["IsFraud"].map({"Yes": 1, "No": 0})
                X = df.drop(columns=["IsFraud"])

                # Detect categorical columns and encode them using LabelEncoder as str
                for col in X.select_dtypes(include="object").columns:
                    le = LabelEncoder()
                    X[col] = le.fit_transform(X[col].astype(str))

                # Balancing samples using RandomUnderSampler
                rus = RandomUnderSampler(random_state=42)
                X_res, y_res = rus.fit_resample(X, y)

                # Train/test split
                X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, stratify=y_res,
                                                                    random_state=42)

                # Feature scaling
                scaler = StandardScaler()
                X_train = scaler.fit_transform(X_train)
                X_test = scaler.transform(X_test)

                model = models[model_name]
                start_time = time.time()
                model.fit(X_train, y_train)
                train_time = time.time() - start_time

                y_pred = model.predict(X_test)
                y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred)
                recall = recall_score(y_test, y_pred)
                roc_auc = roc_auc_score(y_test, y_proba) if y_proba is not None else 0.0
                gini = 2 * roc_auc - 1


                def ks_statistic(y_true, y_proba):
                    from scipy.stats import ks_2samp
                    return ks_2samp(y_proba[y_true == 1], y_proba[y_true == 0]).statistic


                ks = ks_statistic(y_test.to_numpy(), y_proba) if y_proba is not None else 0.0

                st.success(f"‚úÖ {model_name} –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

                st.subheader("üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è (—Ç–µ—Å—Ç)")
                st.markdown(f"‚è± **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:** {train_time:.2f} —Å–µ–∫")
                st.markdown(f"üìä **Gini –∏–Ω–¥–µ–∫—Å:** {gini:.4f}")
                st.markdown(f"üìä **KS –∏–Ω–¥–µ–∫—Å:** {ks:.4f}")
                st.markdown(f"‚úÖ **Accuracy:** {accuracy:.4f}")
                st.markdown(f"üéØ **Precision:** {precision:.4f}")
                st.markdown(f"üîÅ **Recall:** {recall:.4f}")

                y_train_pred = model.predict(X_train)
                y_train_proba = model.predict_proba(X_train)[:, 1] if hasattr(model, "predict_proba") else None
                accuracy_train = accuracy_score(y_train, y_train_pred)
                precision_train = precision_score(y_train, y_train_pred)
                recall_train = recall_score(y_train, y_train_pred)
                roc_auc_train = roc_auc_score(y_train, y_train_proba) if y_train_proba is not None else 0.0
                gini_train = 2 * roc_auc_train - 1
                ks_train = ks_statistic(y_train.to_numpy(), y_train_proba) if y_train_proba is not None else 0.0

                st.subheader("üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è (–æ–±—É—á–µ–Ω–∏–µ)")
                st.markdown(f"‚è± **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:** {train_time:.2f} —Å–µ–∫")
                st.markdown(f"üìä **Gini –∏–Ω–¥–µ–∫—Å:** {gini_train:.4f}")
                st.markdown(f"üìä **KS –∏–Ω–¥–µ–∫—Å:** {ks_train:.4f}")
                st.markdown(f"‚úÖ **Accuracy:** {accuracy_train:.4f}")
                st.markdown(f"üéØ **Precision:** {precision_train:.4f}")
                st.markdown(f"üîÅ **Recall:** {recall_train:.4f}")

                st.subheader("üî¢ –ü—Ä–∏–º–µ—Ä —Ä–∞—Å—á—ë—Ç–æ–≤")
                metrics_df = pd.DataFrame({
                    "–ú–µ—Ç—Ä–∏–∫–∞": ["–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (—Å–µ–∫)", "Gini", "KS", "Accuracy", "Precision", "Recall", "ROC AUC"],
                    "–ó–Ω–∞—á–µ–Ω–∏–µ": [round(train_time, 2), round(gini, 4), round(ks, 4), round(accuracy, 4),
                                 round(precision, 4), round(recall, 4), round(roc_auc, 4)]
                })
                metrics_df_train = pd.DataFrame({
                    "–ú–µ—Ç—Ä–∏–∫–∞": ["–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (—Å–µ–∫)", "Gini", "KS", "Accuracy", "Precision", "Recall", "ROC AUC"],
                    "–ó–Ω–∞—á–µ–Ω–∏–µ": [round(train_time, 2), round(gini_train, 4), round(ks_train, 4),
                                 round(accuracy_train, 4),
                                 round(precision_train, 4), round(recall_train, 4), round(roc_auc_train, 4)]
                })
                st.subheader("üìä –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫ (—Ç–µ—Å—Ç)")
                st.table(metrics_df)

                st.subheader("üìä –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫ (–æ–±—É—á–µ–Ω–∏–µ)")
                st.table(metrics_df_train)

                fig_roc = plt.figure()
                fpr, tpr, _ = roc_curve(y_test, y_proba)
                plt.plot(fpr, tpr, label=f"ROC AUC = {roc_auc:.2f}")
                plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
                plt.xlabel("False Positive Rate")
                plt.ylabel("True Positive Rate")
                plt.title("ROC-–∫—Ä–∏–≤–∞—è")
                plt.legend()
                st.pyplot(fig_roc)

                st.subheader("üîç –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π")
                fig_hist = plt.figure()
                plt.hist(y_proba[y_test == 0], bins=30, alpha=0.6, label="–ö–ª–∞—Å—Å 0 (–Ω–µ –º–æ—à–µ–Ω–Ω–∏–∫)")
                plt.hist(y_proba[y_test == 1], bins=30, alpha=0.6, label="–ö–ª–∞—Å—Å 1 (–º–æ—à–µ–Ω–Ω–∏–∫)")
                plt.title("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø–æ –∫–ª–∞—Å—Å–∞–º")
                plt.xlabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å")
                plt.ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ")
                plt.legend()
                st.pyplot(fig_hist)
                st.markdown(f"**–†–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–π –±–∞–ª–ª (–ø—Ä–∏–º–µ—Ä):** {y_pred[0]}")
                st.markdown(f"**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (–ø—Ä–∏–º–µ—Ä):** {y_proba[0]:.4f}")

                st.subheader("üìå –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ")
                st.code(", ".join(X.columns))

                subset_info = pd.DataFrame({
                    "target": pd.concat([y_train, y_test], ignore_index=True),
                    "subset": ["train"] * len(y_train) + ["test"] * len(y_test)
                })
                st.subheader("üß™ –†–∞–∑–º–µ—Ç–∫–∞ –≤—ã–±–æ—Ä–æ–∫")
                st.dataframe(subset_info.sample(10, random_state=42))

                X_test_raw = pd.DataFrame(scaler.inverse_transform(X_test), columns=X.columns)
                results_df = X_test_raw.copy()
                results_df["–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å"] = y_pred
                results_df["–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å"] = y_proba
                results_df["–ò—Å—Ç–∏–Ω–Ω—ã–π IsFraud"] = y_test.values

                csv = results_df.to_csv(index=False).encode("utf-8")
                col_csv, col_xls = st.columns(2)
                with col_csv:
                    st.download_button(
                        label="üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –≤ CSV",
                        data=csv,
                        file_name="test_predictions.csv",
                        mime="text/csv"
                    )
                with col_xls:
                    import io
                    from openpyxl import Workbook

                    wb = Workbook()
                    ws = wb.active
                    ws.title = "Test Predictions"
                    for col_num, column in enumerate(results_df.columns, 1):
                        ws.cell(row=1, column=col_num, value=column)
                    for row_num, row in enumerate(results_df.values, 2):
                        for col_num, value in enumerate(row, 1):
                            ws.cell(row=row_num, column=col_num, value=value)
                    excel_buffer = io.BytesIO()
                    wb.save(excel_buffer)
                    excel_buffer.seek(0)
                    st.download_button(
                        label="üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –≤ XLS",
                        data=excel_buffer.getvalue(),
                        file_name="test_predictions.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )

                st.success(f"‚úÖ {model_name} –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

                os.makedirs("models", exist_ok=True)

                # Save model
                joblib.dump(model, f"models/{model_name}_model.pkl")

                # Save scaler
                joblib.dump(scaler, f"models/{model_name}_scaler.pkl")

                # Save feature names
                with open(f"models/{model_name}_features.json", "w") as f:
                    json.dump(X.columns.tolist(), f)

    if page == "üîç –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å":
        # Scan all models that have a saved _model.pkl file
        available_models = [
            f.replace("_model.pkl", "")
            for f in os.listdir("models")
            if f.endswith("_model.pkl")
        ]

        if available_models:
            selected_model = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:", available_models)

            model = joblib.load(f"models/{selected_model}_model.pkl")
            scaler = joblib.load(f"models/{selected_model}_scaler.pkl")

            with open(f"models/{selected_model}_features.json", "r") as f:

                feature_names = json.load(f)

            uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª", type=["csv", "xlsx", "xls"])

            if uploaded_file:
                df = pd.read_csv(uploaded_file) if uploaded_file.name.endswith(".csv") else pd.read_excel(uploaded_file)
                df_original = df.copy()

                # Align features (implement align_features if needed)
                df_aligned = align_features(df, feature_names)
                # Encode categorical columns
                for col in df_aligned.select_dtypes(include="object").columns:
                    le = LabelEncoder()
                    df_aligned[col] = le.fit_transform(df_aligned[col].astype(str))
                imputer = SimpleImputer(strategy="mean")
                df_imputed = pd.DataFrame(imputer.fit_transform(df_aligned), columns=df_aligned.columns)
                df_scaled = pd.DataFrame(scaler.transform(df_imputed), columns=df_imputed.columns)

                predictions = model.predict_proba(df_scaled)[:, 1]
                is_fraud_pred = model.predict(df_scaled)
                # Create output DataFrame with only index, –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞, and IsFraud
                output_df = pd.DataFrame({
                    "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞": predictions,
                    "IsFraud": is_fraud_pred
                })
                st.subheader("üìã –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
                rows_to_display = st.slider(
                    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º—ã—Ö —Å—Ç—Ä–æ–∫",
                    min_value=0,
                    max_value=min(100, len(output_df)),
                    value=min(10, len(output_df)),
                    step=1
                )
                st.dataframe(output_df.head(rows_to_display))

                # Two download buttons: CSV and XLS
                col_csv, col_xls = st.columns(2)
                with col_csv:
                    csv_pred = output_df.to_csv(index=False).encode("utf-8")
                    st.download_button(
                        label="üì• –°–∫–∞—á–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ CSV",
                        data=csv_pred,
                        file_name="predictions.csv",
                        mime="text/csv"
                    )
                with col_xls:
                    import io
                    from openpyxl import Workbook

                    wb = Workbook()
                    ws = wb.active
                    ws.title = "Predictions"
                    for col_num, column in enumerate(output_df.columns, 1):
                        ws.cell(row=1, column=col_num, value=column)
                    for row_num, row in enumerate(output_df.values, 2):
                        for col_num, value in enumerate(row, 1):
                            ws.cell(row=row_num, column=col_num, value=value)
                    excel_buffer = io.BytesIO()
                    wb.save(excel_buffer)
                    excel_buffer.seek(0)
                    st.download_button(
                        label="üì• –°–∫–∞—á–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ XLS",
                        data=excel_buffer.getvalue(),
                        file_name="predictions.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )

        else:
            st.error("‚ùå –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å")

if not st.user.is_logged_in:
    st.warning('–î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ AI Sana –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è')